# lung_chatbot_streamlit.py
"""
Single-file Streamlit demo with robust Grad-CAM visualization.

- Upload .jpg CT slices (grayscale) or a .npy CT volume
- Run inference with a Keras model (resnet50_lung_cancer.h5)
- Show Grad-CAM overlay + annotated bounding box for a representative slice
- Kannada chatbot with TTS
"""

import re
import os
import io
import time
import math
import tempfile
import traceback
import requests
import streamlit as st
import numpy as np
from PIL import Image
import cv2
from gtts import gTTS

import tensorflow as tf
from tensorflow.keras.models import load_model as keras_load_model

# ---------- CONFIG ----------
MODEL_PATH = os.getenv("MODEL_PATH", "/app/resnet50_lung_cancer.h5")
MODEL_DRIVE_ID = os.getenv("MODEL_DRIVE_ID")  # set this in Render / docker env
INPUT_SIZE = (224, 224)
CLASS_MAP = {0: "Normal", 1: "Benign", 2: "Malignant"}
# ----------------------------

st.set_page_config(page_title="Lung Cancer Demo Chatbot (Kannada)", layout="wide")

# GPU memory growth (works if GPU present locally)
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            try:
                tf.config.experimental.set_memory_growth(gpu, True)
            except Exception:
                pass
        print("Enabled memory growth for GPUs")
    except Exception as e:
        print("Could not set memory growth:", e)


# Try loading logo from a few sensible relative locations (works in Docker)
logo = None
for candidate in ["imagel.png", "assets/imagel.png", "logo2.png", "assets/logo2.png", "logo.png", "assets/logo.png"]:
    try:
        if os.path.exists(candidate):
            logo = Image.open(candidate)
            break
    except Exception:
        logo = None
        break

if logo is not None:
    try:
        st.image(logo, width=150)
    except Exception:
        pass


# ---------- Robust downloader (stream + retries + HDF5 check) ----------
HDF5_MAGIC = b"\x89HDF\r\n\x1a\n"


def _is_hdf5_file(path, nbytes=8):
    try:
        with open(path, "rb") as f:
            head = f.read(nbytes)
        return head.startswith(HDF5_MAGIC)
    except Exception:
        return False


def free_space_mb(path="/"):
    try:
        stv = os.statvfs(path)
        return (stv.f_bavail * stv.f_frsize) // (1024 * 1024)
    except Exception:
        return 0


def download_from_drive_stream(drive_id, out_path, max_retries=4, chunk_size=32768):
    """
    Robust download from Google Drive.
    - Handles confirm token cookie (download_warning) and parses confirm token from HTML links.
    - Verifies HDF5 signature after download.
    - Falls back to gdown if streaming approach yields an HTML/quota page.
    Returns (True, None) on success or (False, reason_str) on failure.
    """
    base_url = f"https://drive.google.com/uc?export=download&id={drive_id}"
    session = requests.Session()

    for attempt in range(1, max_retries + 1):
        try:
            st.write(f"Download attempt {attempt} — free disk: {free_space_mb()} MB")
            if free_space_mb() and free_space_mb() < 200:
                st.error(f"Low disk space (<200MB). Free: {free_space_mb()} MB")
                return False, "low_disk"

            resp = session.get(base_url, stream=True, timeout=30)
            content_type = resp.headers.get("Content-Type", "")

            # If Drive returned HTML (confirm / quota / virus page), try to extract confirm token
            confirm_token = None
            if 'text/html' in content_type.lower():
                text = resp.text
                st.write("Drive returned HTML; trying to find confirm token/cookie...")

                # 1) cookie-based token (commonly 'download_warning')
                for k, v in resp.cookies.items():
                    if k.startswith("download_warning") or k.startswith("confirm"):
                        confirm_token = v
                        st.write(f"Found confirm token in cookies: {k}={confirm_token}")
                        break

                # 2) fallback: parse HTML for confirm param in links/buttons
                if confirm_token is None:
                    m = re.search(r"confirm=([0-9A-Za-z_\-]+)&", text)
                    if m:
                        confirm_token = m.group(1)
                        st.write("Found confirm token in HTML (regex).")
                    else:
                        m2 = re.search(r"uc-download-link\" href=\"[^\"]*confirm=([0-9A-Za-z_\-]+)", text)
                        if m2:
                            confirm_token = m2.group(1)
                            st.write("Found confirm token in HTML (uc-download-link).")

                # If we found a token, request again with confirm
                if confirm_token:
                    confirm_url = base_url + f"&confirm={confirm_token}"
                    st.write("Requesting confirmed URL...")
                    resp = session.get(confirm_url, stream=True, timeout=30)
                else:
                    st.write("No confirm token found in HTML. Proceeding — may get a small HTML file.")

            if resp.status_code != 200:
                st.error(f"Drive HTTP {resp.status_code}")
                return False, f"http_{resp.status_code}"

            tmp_path = out_path + ".part"
            total = resp.headers.get('Content-Length')
            total = int(total) if total and total.isdigit() else None
            written = 0
            start = time.time()
            with open(tmp_path, "wb") as f:
                for chunk in resp.iter_content(chunk_size=chunk_size):
                    if chunk:
                        f.write(chunk)
                        written += len(chunk)
                        if total:
                            pct = min(1.0, written / total)
                            try:
                                st.progress(pct)
                            except Exception:
                                pass
                            st.write(f"{written//1024//1024} MB / {math.ceil(total/1024/1024)} MB")
                        else:
                            if written % (1024*1024*10) < chunk_size:
                                st.write(f"Downloaded {written//1024//1024} MB ...")

            os.replace(tmp_path, out_path)
            elapsed = time.time() - start
            st.write(f"Attempt {attempt} complete — {written//1024//1024} MB in {elapsed:.1f}s")

            # Verify HDF5 signature
            if _is_hdf5_file(out_path):
                st.success("Downloaded file looks like a valid HDF5 (.h5) file.")
                return True, None
            else:
                st.warning("Downloaded file is NOT a valid HDF5 file (likely HTML or corrupted). Will try gdown fallback.")

                # attempt gdown fallback (often handles Drive confirm pages)
                try:
                    import gdown
                    st.write("Running gdown fallback...")
                    gdown_url = f"https://drive.google.com/uc?id={drive_id}"
                    gdown.download(gdown_url, out_path, quiet=False)
                    if _is_hdf5_file(out_path):
                        st.success("gdown fallback succeeded and file looks like HDF5.")
                        return True, None
                    else:
                        st.error("gdown downloaded file but it is not valid HDF5 either.")
                        try:
                            os.remove(out_path)
                        except Exception:
                            pass
                        return False, "not_hdf5_after_gdown"
                except Exception as e:
                    st.error(f"gdown fallback failed: {e}")
                    try:
                        if os.path.exists(out_path):
                            os.remove(out_path)
                    except Exception:
                        pass
                    return False, f"gdown_error:{e}"

        except requests.exceptions.RequestException as e:
            st.warning(f"Network error on attempt {attempt}: {e}")
            time.sleep(2 ** attempt)
        except Exception as e:
            st.error(f"Unexpected error: {e}")
            try:
                if os.path.exists(out_path):
                    os.remove(out_path)
            except Exception:
                pass
            return False, str(e)

    return False, "max_retries"


# ---------- Model loader (cached) ----------
@st.cache_resource(show_spinner=True)
def load_keras_model():
    """
    Ensure model exists at MODEL_PATH; if missing and MODEL_DRIVE_ID provided, try to download.
    Returns (model_or_none, message_str)
    """
    try:
        # If model file already present
        if os.path.exists(MODEL_PATH):
            return keras_load_model(MODEL_PATH, compile=False), f"Loaded model from: {MODEL_PATH}"

        # If MODEL_DRIVE_ID is set, try to download
        if MODEL_DRIVE_ID:
            st.info(f"Model not found. Downloading from Drive id: {MODEL_DRIVE_ID} (this may take a while)...")
            ok, reason = download_from_drive_stream(MODEL_DRIVE_ID, MODEL_PATH, max_retries=4)
            if not ok:
                st.error(f"Stream download failed: {reason}. Attempting fallback using gdown...")
                try:
                    import gdown
                    url = f"https://drive.google.com/uc?id={MODEL_DRIVE_ID}"
                    gdown.download(url, MODEL_PATH, quiet=False)
                    if not os.path.exists(MODEL_PATH):
                        st.error("gdown fallback also failed.")
                        return None, "gdown_failed"
                except Exception as e:
                    st.error(f"gdown fallback error: {e}")
                    return None, f"gdown_error: {e}"
        else:
            return None, f"Model not found at: {MODEL_PATH}. Set MODEL_DRIVE_ID env var or place model file in app folder."

        # final check and load
        if not os.path.exists(MODEL_PATH):
            return None, f"Model missing after download attempt: {MODEL_PATH}"

        model = keras_load_model(MODEL_PATH, compile=False)
        return model, f"Loaded model from: {MODEL_PATH}"

    except Exception as e:
        # remove possibly corrupted file
        try:
            if os.path.exists(MODEL_PATH):
                os.remove(MODEL_PATH)
        except Exception:
            pass
        return None, f"Error loading model: {e}"


# --- Preprocess helpers ---
def preprocess_slice(slice_img, target_size=INPUT_SIZE):
    img = slice_img.astype(np.float32)
    if img.ndim == 2:
        img = np.stack([img, img, img], axis=-1)
    img = cv2.resize(img, target_size)
    img = img / 255.0
    return img.astype(np.float32)


# --- Find last conv layer robustly ---
def find_last_conv_layer(model):
    for layer in reversed(model.layers):
        name = layer.name.lower()
        if 'conv' in name:
            return layer.name
        if layer.__class__.__name__.lower().startswith('conv'):
            return layer.name
    raise ValueError("No convolutional layer found in model to compute Grad-CAM.")


# --- Grad-CAM helper (robust to multi-output models) ---
def make_gradcam_heatmap(img_array, model, pred_index=None, last_conv_layer_name=None):
    if last_conv_layer_name is None:
        last_conv_layer_name = find_last_conv_layer(model)

    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)

        if isinstance(predictions, (list, tuple)):
            predictions = predictions[0]

        predictions = tf.convert_to_tensor(predictions)

        if pred_index is None:
            pred_index = tf.argmax(predictions[0])

        loss = predictions[:, pred_index]

    grads = tape.gradient(loss, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    conv_outputs = conv_outputs[0]

    weighted = conv_outputs * pooled_grads[tf.newaxis, tf.newaxis, :]
    heatmap = tf.reduce_sum(weighted, axis=-1)

    heatmap = np.maximum(heatmap, 0)
    max_val = tf.reduce_max(heatmap)
    if max_val == 0:
        return np.zeros((heatmap.shape[0], heatmap.shape[1]), dtype=np.float32)
    heatmap /= max_val
    return heatmap.numpy().astype(np.float32)


# --- Overlay helper ---
def overlay_heatmap_on_image(orig_rgb, heatmap, thresh=0.35):
    if orig_rgb.dtype != np.uint8:
        orig = (255 * (orig_rgb - orig_rgb.min()) / (orig_rgb.ptp() + 1e-8)).astype(np.uint8)
    else:
        orig = orig_rgb.copy()

    hmap_u8 = (heatmap * 255).astype(np.uint8)
    hmap_resized = cv2.resize(hmap_u8, (orig.shape[1], orig.shape[0]))
    heatmap_color = cv2.applyColorMap(hmap_resized, cv2.COLORMAP_JET)  # BGR
    overlay_bgr = cv2.addWeighted(cv2.cvtColor(orig, cv2.COLOR_RGB2BGR), 0.6, heatmap_color, 0.4, 0)
    overlay_rgb = cv2.cvtColor(overlay_bgr, cv2.COLOR_BGR2RGB)

    _, mask = cv2.threshold(hmap_resized, int(thresh * 255), 255, cv2.THRESH_BINARY)
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    annotated_bgr = cv2.cvtColor(orig, cv2.COLOR_RGB2BGR)
    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)
        if w * h < 50:
            continue
        cv2.rectangle(annotated_bgr, (x, y), (x + w, y + h), (0, 0, 255), 2)
    annotated_rgb = cv2.cvtColor(annotated_bgr, cv2.COLOR_BGR2RGB)

    return overlay_rgb, annotated_rgb


# --- Robust predict handling + Grad-CAM pipeline ---
def run_inference_with_gradcam(model, volume_array, return_gradcam=True):
    try:
        if model is None:
            mean_val = float(np.nanmean(volume_array))
            score = 1.0 / (1.0 + np.exp(-0.01 * (mean_val - 100)))
            label = "ಸ್ಥೂಲ ಸಂಶಯ (Malignant)" if score > 0.5 else "ಸಂದೇಹದಿಲ್ಲ (Benign)"
            return {"label": label, "score": float(score), "probs": None, "notes": "Demo inference; replace with real model.", "gradcam": None}

        arr = np.squeeze(volume_array)
        if arr.ndim == 2:
            arr = arr[np.newaxis, ...]
        S = arr.shape[0]

        preproc_list = []
        orig_resized = []
        for i in range(S):
            sl = arr[i]
            if sl.ndim == 2:
                rgb = np.stack([sl, sl, sl], axis=-1)
            else:
                rgb = sl
            rgb_resized = cv2.resize(rgb.astype(np.uint8), INPUT_SIZE)
            orig_resized.append(rgb_resized)
            x = preprocess_slice(sl, target_size=INPUT_SIZE)
            preproc_list.append(x)

        X = np.stack(preproc_list, axis=0).astype(np.float32)

        preds = model.predict(X, batch_size=16)
        if isinstance(preds, (list, tuple)):
            preds = preds[0]
        preds = np.asarray(preds)
        if preds.ndim == 1:
            preds = np.expand_dims(preds, axis=0)
        if preds.ndim != 2:
            return {"error": f"Unexpected prediction shape: {preds.shape}", "trace": None, "gradcam": None}
        row_sums = preds.sum(axis=1)
        if not np.allclose(row_sums, 1.0, atol=1e-3):
            preds = tf.nn.softmax(preds, axis=-1).numpy()

        probs = preds
        mean_probs = probs.mean(axis=0)
        class_idx = int(np.argmax(mean_probs))
        score = float(mean_probs[class_idx])

        label_en = CLASS_MAP.get(class_idx, f"class_{class_idx}")
        kn_map = {"Normal": "ಸಾಮಾನ್ಯ (Normal)", "Benign": "ಸಂದೇಹವಿಲ್ಲ (Benign)", "Malignant": "ಸ್ಥೂಲ ಸಂಶಯ (Malignant)"}
        label_kn = kn_map.get(label_en, label_en)

        gradcam_info = None
        if return_gradcam:
            per_slice_scores = probs[:, class_idx]
            rep_idx = int(np.argmax(per_slice_scores))
            if rep_idx < 0 or rep_idx >= len(preproc_list):
                rep_idx = 0
            single_input = np.expand_dims(preproc_list[rep_idx], axis=0).astype(np.float32)
            try:
                last_conv = None
                try:
                    last_conv = find_last_conv_layer(model)
                except Exception as e:
                    st.info(f"Last conv detection: {e}")
                    last_conv = None
                heatmap = make_gradcam_heatmap(single_input, model, pred_index=class_idx, last_conv_layer_name=last_conv)
                if heatmap is None or heatmap.size == 0:
                    raise RuntimeError("Heatmap empty")
                overlay_rgb, annotated_rgb = overlay_heatmap_on_image(orig_resized[rep_idx], heatmap, thresh=0.35)
                gradcam_info = {"slice_index": rep_idx, "overlay_rgb": overlay_rgb, "annotated_rgb": annotated_rgb}
            except Exception as e:
                gradcam_info = {"error": str(e), "trace": traceback.format_exc()}

        return {"label": label_kn, "score": score, "probs": mean_probs.tolist(), "notes": "Model-based inference (mean over slices).", "gradcam": gradcam_info}

    except Exception as e:
        return {"error": str(e), "trace": traceback.format_exc(), "gradcam": None}


# --- Kannada response templates & NLU ---
KANNADA_TEMPLATES = {
    "greeting": "ನಮಸ್ತೆ! 나는 ನಿಮ್ಮ ಲಂಗ್-ಕ್ಯಾನ್ಸರ್ ಪ್ರೋಟೋಟೈಪ್ ಚಾಟ್‌ಬಾಟ್. ನಿಮಗೆ ಹೇಗೆ ಸಹಾಯ ಮಾಡಬಹುದು?",
    "inference_result": "ಮೌಲ್ಯಮಾಪನ ಫಲಿತಾಂಶ:\n\nಫಲಿತಾಂಶ: {label}\nconfidence (ಸ್ಕೋರ್): {score:.3f}\nನೋಟ್: {notes}",
    "explain_how": "ಈ ಮಾಡೆಲ್ 3D CT ವಾಲ್ಯೂಮ್ ಗಳನ್ನು ಒಳಗೆ ತೆಗೆದುಕೊಂಡು ಕಂಡುಬರುವ ಗುಣಲಕ್ಷಣಗಳನ್ನು ಆಧರಿಸಿ ಅನುಮಾನಿತ तಂತು/ನೋಡ್ ಗಳನ್ನು ಗುರುತಿಸುತ್ತದೆ.",
    "accuracy": "ಮಾಡೆಲ್‌ಗಾಗಿ ಉದಾಹರಣೆ accuracy = {acc:.2f}.",
    "limitations": "ಸೀಮಿತತೆ: ಇದು ಡೆಮೊ; ವೈದ್ಯ ಸಲಹೆ ಅವಶ್ಯಕ.",
    "thanks": "ಧನ್ಯವಾದಗಳು!"
}


def detect_intent(user_text):
    t = user_text.strip().lower()
    if any(x in t for x in ["hello", "hi", "namaste", "ನಮಸ್ತೆ", "ಹಲೋ", "hey"]):
        return "greeting"
    if any(x in t for x in ["predict", "inference", "run inference", "result", "ಫಲ", "ಫಲಿತಾಂಶ"]):
        return "predict"
    if any(x in t for x in ["how", "work", "ಹೇಗೆ", "ಮಾಡುತ್ತದೆ", "explain", "ವಿವರಣೆ"]):
        return "explain"
    if any(x in t for x in ["accuracy", "precision", "sensitivity", "acc"]):
        return "accuracy"
    if any(x in t for x in ["limitations", "limits", "ಸೀಮಿತತೆ"]):
        return "limitations"
    if any(x in t for x in ["thanks", "thank", "ಧನ್ಯ", "bye"]):
        return "thanks"
    return "unknown"


def answer_in_kannada(intent, context=None):
    if intent == "greeting":
        return KANNADA_TEMPLATES["greeting"]
    if intent == "explain":
        return KANNADA_TEMPLATES["explain_how"]
    if intent == "accuracy":
        acc = context.get("acc", 0.85) if context else 0.85
        return KANNADA_TEMPLATES["accuracy"].format(acc=acc)
    if intent == "limitations":
        return KANNADA_TEMPLATES["limitations"]
    if intent == "thanks":
        return KANNADA_TEMPLATES["thanks"]
    if intent == "predict":
        return "ದಯವಿಟ್ಟು JPG ಸ್ಲೈಸುಗಳನ್ನು ಅಥವಾ .npy ಫೈಲ್ ಅನ್ನು ಅಪ್‌ಲೋಡ್ ಮಾಡಿ ಮತ್ತು 'Run Inference' ಒತ್ತಿ."
    return "ಕ್ಷಮಿಸಿ, ನನಗೆ ಅರ್ಥವಾಗಲಿಲ್ಲ."


# --- TTS helper ---
def tts_kannada(text):
    try:
        tts = gTTS(text=text, lang='kn')
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
        tts.save(tmp.name)
        return tmp.name
    except Exception as e:
        st.error("TTS error: " + str(e))
        return None


# --- Streamlit UI ---
st.title("Lung Cancer Detector")
st.markdown("*DISCLAIMER:* This is a demo prototype. Not a medical diagnosis tool.*")

col1, col2 = st.columns([1, 1])

with col1:
    st.header("Model / Inference")
    model, model_msg = load_keras_model()
    if model is not None:
        st.success(model_msg)
    else:
        st.info(model_msg)

    uploaded_files = st.file_uploader("Upload CT scan slices (.jpg) or a single .npy", type=["jpg", "jpeg", "npy"], accept_multiple_files=True)
    arr = None

    if uploaded_files:
        npy_files = [f for f in uploaded_files if f.name.lower().endswith('.npy')]
        jpg_files = [f for f in uploaded_files if f.name.lower().endswith(('.jpg', '.jpeg'))]

        if len(npy_files) == 1 and len(uploaded_files) == 1:
            arr = np.load(io.BytesIO(npy_files[0].read()))
            st.write("Loaded .npy volume shape:", arr.shape)
            st.session_state["last_volume"] = arr
        elif len(jpg_files) >= 1:
            jpg_files = sorted(jpg_files, key=lambda x: x.name)
            imgs = []
            for file in jpg_files:
                img = Image.open(file).convert("L")
                imgs.append(np.array(img))
            arr = np.stack(imgs, axis=0)
            st.write("Stacked JPG volume shape:", arr.shape)
            st.session_state["last_volume"] = arr
        else:
            st.warning("Upload either a single .npy file or one/more JPG slices.")
    else:
        arr = st.session_state.get("last_volume", None)

    if st.button("Run Inference"):
        if arr is None:
            st.warning("Please upload JPG slices or a .npy volume first.")
        else:
            with st.spinner("Running inference + Grad-CAM..."):
                result = run_inference_with_gradcam(model, arr, return_gradcam=True)
                if "error" in result:
                    st.error("Inference failed: " + result.get("error", "unknown"))
                    if result.get("trace"):
                        st.text(result.get("trace"))
                else:
                    out_text = KANNADA_TEMPLATES["inference_result"].format(label=result.get("label","N/A"), score=result.get("score",0.0), notes=result.get("notes",""))
                    st.markdown("### ಫಲಿತಾಂಶ (Kannada)")
                    st.code(out_text)

                    audio_file = tts_kannada(out_text)
                    if audio_file:
                        st.audio(open(audio_file, "rb").read(), format="audio/mp3", autoplay=True)
                        st.session_state["last_bot_audio"] = audio_file

                    st.session_state["last_result"] = result

                    gc = result.get("gradcam")
                    if gc is None:
                        st.info("No Grad-CAM (model missing or disabled).")
                    elif isinstance(gc, dict) and "error" in gc:
                        st.error("Grad-CAM generation failed: " + gc.get("error", ""))
                        if gc.get("trace"):
                            st.text(gc.get("trace"))
                    elif isinstance(gc, dict) and "overlay_rgb" in gc:
                        rep_idx = gc.get("slice_index", None)
                        st.markdown(f"**Representative slice index:** {rep_idx}")
                        c1, c2 = st.columns(2)
                        with c1:
                            st.image(gc["annotated_rgb"], caption="Original slice with bounding box (red)", use_container_width=True)
                        with c2:
                            st.image(gc["overlay_rgb"], caption="Grad-CAM overlay (heatmap)", use_container_width=True)
                    else:
                        st.info("Grad-CAM: unexpected format; see logs.")

with col2:
    st.header("Chat (Kannada)")
    st.markdown("Type questions in Kannada or English; the bot replies in Kannada (with speech).")

    if "chat_history" not in st.session_state:
        st.session_state["chat_history"] = []

    user_input = st.text_input("Your question (Kannada/English):", key="user_input")
    if st.button("Send"):
        if user_input.strip() == "":
            st.warning("Please type a question.")
        else:
            st.session_state["chat_history"].append(("user", user_input))
            intent = detect_intent(user_input)

            if intent == "predict":
                if "last_result" in st.session_state:
                    r = st.session_state["last_result"]
                    bot_text = KANNADA_TEMPLATES["inference_result"].format(label=r.get("label","N/A"), score=r.get("score",0.0), notes=r.get("notes",""))
                else:
                    bot_text = answer_in_kannada(intent)
            elif intent == "accuracy":
                bot_text = answer_in_kannada(intent, context={"acc":0.92})
            else:
                bot_text = answer_in_kannada(intent)

            st.session_state["chat_history"].append(("bot", bot_text))
            audio_path = tts_kannada(bot_text)
            if audio_path:
                st.audio(open(audio_path, "rb").read(), format="audio/mp3", autoplay=True)
                st.session_state["last_bot_audio"] = audio_path

    for speaker, text in st.session_state["chat_history"][-20:]:
        if speaker == "user":
            st.markdown(f"*You:* {text}")
        else:
            st.markdown(f"*Bot:* {text}")
